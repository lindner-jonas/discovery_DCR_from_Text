{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d20d1c",
   "metadata": {},
   "source": [
    "# Meta Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'process_text' in locals():\n",
    "    process_text = \"\"\n",
    "\n",
    "if not 'process_name' in locals():\n",
    "    process_name = \"\"\n",
    "\n",
    "if not 'large_language_model' in locals():\n",
    "    large_language_model = \"\"\n",
    "\n",
    "if not 'prompt_name' in locals():\n",
    "    prompt_name = \"\"\n",
    "\n",
    "if not 'loop_limit' in locals():\n",
    "    loop_limit = \"20\"\n",
    "\n",
    "if not 'llm_url' in locals():\n",
    "    llm_url = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "# comp-gpu06:8765\n",
    "\n",
    "# print(\"process_name:\", process_name)\n",
    "# print(\"process description: \", process_text)\n",
    "# print(\"llm:\" , large_language_model)\n",
    "# print(\"prompt_name:\", prompt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cccece",
   "metadata": {},
   "source": [
    "# Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Context\n",
    "prompt_Role = \"\"\"\n",
    "[Context]\n",
    "Imaging you are an expert in Business Process Management with extensive knowledge about declarative process modelling. Additionally, you have specific knowledge about the modelling technique \"DCR\" (Dynamic Condition Response).\n",
    "I am going to feed you a textual description of some sort of Business Process. It could be a process in a company or organization. \n",
    "\"\"\"\n",
    "\n",
    "#Task\n",
    "prompt_Role = prompt_Role + \"\"\"\n",
    "{{ task }}\n",
    "\"\"\"\n",
    "task = \"\"\"\n",
    "Your task is to find all roles or actors for a DCR graph in the process description I will provide you with.\n",
    "\"\"\"\n",
    "\n",
    "# Output / Format\n",
    "prompt_Role = prompt_Role + \"\"\"\n",
    "Return the roles in an array in JSON format, similar to: {\"roles\": [...,...,...]}. Do not forget the delimiter \",\" between the identified roles. Do not use any other delimiter. Do not add any other information to your answer. Do not include duplicate roles. Do not repeat the task description in your answer. Do neither describe nor explain your answer. \n",
    "\"\"\"\n",
    "\n",
    "# Process Description\n",
    "prompt_Role = prompt_Role + \"\"\"\n",
    "The process description in question is:\n",
    "{{ process }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a5dc2",
   "metadata": {},
   "source": [
    "## Combine Role Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd71b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fcd339ba650>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OllamaGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_builder = PromptBuilder(template=prompt_Role)\n",
    "llm = OllamaGenerator(model=large_language_model, url=llm_url, timeout=1200)\n",
    "\n",
    "role_pipeline = Pipeline()\n",
    "role_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "role_pipeline.add_component(\"llm\", llm)\n",
    "role_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11cd18",
   "metadata": {},
   "source": [
    "# Prompt for Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Context\n",
    "prompt_Activity = \"\"\"\n",
    "Imaging you are an expert in Business Process Management with extensive knowledge about declarative process modelling. Additionally, you have specific knowledge about the modelling technique \"DCR\" (Dynamic Condition Response).\n",
    "I am going to feed you a textual description of some sort of Business Process. It could be a process in a company or organization. \n",
    "\"\"\"\n",
    "\n",
    "#Task\n",
    "prompt_Activity = prompt_Activity + \"\"\"\n",
    "Your task is to find all activities or events for a DCR graph in the process description I will provide you with and map them to the following roles: {{ roles }}.\n",
    "\"\"\"\n",
    "\n",
    "# Output / Format\n",
    "prompt_Activity = prompt_Activity + \"\"\"\n",
    "Return the activitites as keys in key-value-pairs in JSON format, with their associated role as value, which should look like this: { {\"activity\": ..., \"role\": ...}, ...}. Do not forget the delimiter \",\" between the identified activities. Do not use any other delimiter. Do not alter the text. Do not add any other information to your answer. Do not include duplicate activities. Do not repeat the task description in your answer. Do neither describe nor explain your answer. \n",
    "\"\"\"\n",
    "\n",
    "# Process Description\n",
    "prompt_Activity = prompt_Activity + \"\"\"\n",
    "The process description in question is:\n",
    "{{ process }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78433451",
   "metadata": {},
   "source": [
    "## Combine Activity Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dccee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fcd339bad70>\n",
       "ðŸš… Components\n",
       "  - prompt_builder_Activity: PromptBuilder\n",
       "  - llm2: OllamaGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder_Activity.prompt -> llm2.prompt (str)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_builder_Activity = PromptBuilder(template=prompt_Activity)\n",
    "llm2 = OllamaGenerator(model=large_language_model, url=llm_url, timeout=1200)\n",
    "\n",
    "activity_pipeline = Pipeline()\n",
    "activity_pipeline.add_component(\"prompt_builder_Activity\", prompt_builder_Activity)\n",
    "activity_pipeline.add_component(\"llm2\", llm2)\n",
    "activity_pipeline.connect(\"prompt_builder_Activity\", \"llm2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422dc56",
   "metadata": {},
   "source": [
    "# Prompt for Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Context\n",
    "prompt_Rules = \"\"\"\n",
    "Imaging you are an expert in Business Process Management with extensive knowledge about declarative process modelling. Additionally, you have specific knowledge about the modelling technique \"DCR\" (Dynamic Condition Response).\n",
    "I am going to feed you a textual description of some sort of Business Process. It could be a process in a company or organization. \n",
    "\"\"\"\n",
    "\n",
    "#Task\n",
    "prompt_Rules = prompt_Rules + \"\"\"\n",
    "Your task is to find and define all constraints between the activities for a DCR graph in the process description I will provide you with, based on the already identified activities: {{ activities }}.\n",
    "A constraint must be in accordance with DCR a Condition, Response, Exclude, Include or Milestone, and its associated activities are a source activity and a target activity.\n",
    "\"\"\"\n",
    "\n",
    "# Output / Format\n",
    "prompt_Rules = prompt_Rules + \"\"\"\n",
    "Return the DCR constraints as keys in key-value-pairs in JSON format, which should look like this: { {\"constraint\": ..., \"source activity: ..., \"target activity\": ...}, ...}. The constraint should be one of the constraint types of DCR and the activities its associated source and target activity. Do not forget the delimiter \",\" between the identified constraints. Do not use any other delimiter. Do not alter the text. Do not add any other information to your answer. Do not include duplicate constraints. Do not include the roles of the activities in the output. Do not repeat the task description in your answer. Do neither describe nor explain your answer. \n",
    "\"\"\"\n",
    "\n",
    "# Process Description\n",
    "prompt_Rules = prompt_Rules + \"\"\"\n",
    "The process description in question is:\n",
    "{{ process }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c344bd",
   "metadata": {},
   "source": [
    "## Combine Rule Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e7177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fcd33b4b410>\n",
       "ðŸš… Components\n",
       "  - prompt_builder_Rules: PromptBuilder\n",
       "  - llm3: OllamaGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder_Rules.prompt -> llm3.prompt (str)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack import component\n",
    "\n",
    "prompt_builder_Rules = PromptBuilder(template=prompt_Rules)\n",
    "llm3 = OllamaGenerator(model=large_language_model, url=llm_url, timeout=1200)\n",
    "\n",
    "rule_pipeline = Pipeline()\n",
    "rule_pipeline.add_component(\"prompt_builder_Rules\", prompt_builder_Rules)\n",
    "rule_pipeline.add_component(\"llm3\", llm3)\n",
    "rule_pipeline.connect(\"prompt_builder_Rules\", \"llm3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae021f",
   "metadata": {},
   "source": [
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET \n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef794734",
   "metadata": {},
   "source": [
    "## Run Prompt Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324873d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPromptPipeline(): \n",
    "    start_time_role = datetime.now()\n",
    "    response_Roles = role_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder\": {\"task\": task, \"process\": process_text},\n",
    "        }\n",
    "    )\n",
    "    processing_time_role = datetime.now() - start_time_role\n",
    "    processing_duration_role = processing_time_role.total_seconds()\n",
    "    input_tokens_Role = response_Roles[\"llm\"][\"meta\"][0][\"prompt_eval_count\"]\n",
    "    output_tokens_Role = response_Roles[\"llm\"][\"meta\"][0][\"eval_count\"]\n",
    "    cleaned_roles = ''.join(response_Roles[\"llm\"][\"replies\"])\n",
    "    cleaned_roles = cleaned_roles.replace(\"'\", '').replace(\"{\", '').replace(\"}\", '').replace(\"[\", '').replace(\"]\", '').replace(\":\", '').replace(\"\\\"roles\\\"\", '').replace(\"\\n\",'').replace(\" \", '').replace(\"\\\",\\\"\",'\\\", \\\"').replace(\"\\\\\",'')\n",
    "\n",
    "    start_time_activity = datetime.now()\n",
    "    response_Activities = activity_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder_Activity\": {\"process\": process_text, \"roles\": cleaned_roles},\n",
    "        }\n",
    "    )\n",
    "    processing_time_activity = datetime.now() - start_time_activity\n",
    "    processing_duration_activity = processing_time_activity.total_seconds()\n",
    "    input_tokens_Activity = response_Activities[\"llm2\"][\"meta\"][0][\"prompt_eval_count\"]\n",
    "    output_tokens_Activity = response_Activities[\"llm2\"][\"meta\"][0][\"eval_count\"]\n",
    "    cleaned_activities = ''.join(response_Activities[\"llm2\"][\"replies\"])\n",
    "    cleaned_activities = cleaned_activities.replace(\"'\", '').replace(\"[\", '').replace(\"]\", '').replace(\"\\\"roles\\\"\", '').replace(\"\\n\",'').replace(\"\\\",\\\"\",'\\\", \\\"').replace(\"\\\\\",'').replace('}{\"activity','},{\"activity').replace('} {\"activity','},{\"activity')\n",
    "\n",
    "    start_time_rule = datetime.now()\n",
    "    response_Rules = rule_pipeline.run(\n",
    "        {\n",
    "            \"prompt_builder_Rules\": {\"process\": process_text, \"activities\": cleaned_activities},#{\"question\": question},\n",
    "        }\n",
    "    )\n",
    "    processing_time_rule = datetime.now() - start_time_rule\n",
    "    processing_duration_rule = processing_time_rule.total_seconds()\n",
    "    input_tokens_Rule = response_Rules[\"llm3\"][\"meta\"][0][\"prompt_eval_count\"]\n",
    "    output_tokens_Rule = response_Rules[\"llm3\"][\"meta\"][0][\"eval_count\"]\n",
    "    cleaned_rules = ''.join(response_Rules[\"llm3\"][\"replies\"])\n",
    "    cleaned_rules = cleaned_rules.replace(\"'\", '').replace(\"[\", '').replace(\"]\", '').replace(\"\\n\",'').replace(\"  \",'').replace(\"\\\",\\\"\",'\\\", \\\"').replace(\"\\\\\",'').replace('}{\"constraint','},{\"constraint').replace('} {\"constraint','},{\"constraint')\n",
    "\n",
    "    return cleaned_roles, processing_duration_role, input_tokens_Role, output_tokens_Role, cleaned_activities, processing_duration_activity, input_tokens_Activity, output_tokens_Activity, cleaned_rules, processing_duration_rule, input_tokens_Rule, output_tokens_Rule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc115d",
   "metadata": {},
   "source": [
    "## Create JSON from Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateJSON(roles,activities,rules):\n",
    "    rolesString = '\"roles\": [' + roles + ']'\n",
    "    activitiesString = '\"activities\": [' + activities + ']'\n",
    "    rulesString = '\"constraints\": [' + rules + ']'\n",
    "    graphString = '{\"DCRgraph\": {' + rolesString + ', ' + activitiesString + ', ' + rulesString + '} }'\n",
    "    print(graphString, flush=True)\n",
    "    graphJSON = json.loads(graphString)\n",
    "    \n",
    "    # print(rolesString)\n",
    "    # print(activitiesString)\n",
    "    # print(rulesString)\n",
    "    # print(graphString)\n",
    "\n",
    "    return graphJSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfb5d7",
   "metadata": {},
   "source": [
    "## Generate DCR Editor XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324be2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the code for json_to_xml() was mostly generated using GPT-4o mini\n",
    "def json_to_xml(json_obj):\n",
    "    # Create the root element\n",
    "    definitions = ET.Element(\"dcr:definitions\", {\n",
    "        \"xmlns:dcr\": \"http://tk/schema/dcr\",\n",
    "        \"xmlns:dcrDi\": \"http://tk/schema/dcrDi\",\n",
    "        \"xmlns:dc\": \"http://www.omg.org/spec/DD/20100524/DC\"\n",
    "    })\n",
    "\n",
    "    # Create dcrGraph element\n",
    "    dcr_graph = ET.SubElement(definitions, \"dcr:dcrGraph\", id=\"dcrGraph\")\n",
    "\n",
    "    # Create events\n",
    "    event_ids = {}\n",
    "    for activity in json_obj[\"DCRgraph\"][\"activities\"]:\n",
    "        role = activity[\"role\"]\n",
    "        description = activity[\"activity\"]\n",
    "        event_id = f\"Event_{uuid.uuid4().hex[:8]}\"  # Generate a unique event ID\n",
    "        event_ids[event_id] = activity  # Store the event ID with its activity for later use\n",
    "\n",
    "        event = ET.SubElement(dcr_graph, \"dcr:event\", {\n",
    "            \"id\": event_id,\n",
    "            \"role\": role,\n",
    "            \"description\": description,\n",
    "            \"included\": \"true\",\n",
    "            \"executed\": \"false\",\n",
    "            \"pending\": \"false\",\n",
    "            \"enabled\": \"false\"\n",
    "        })\n",
    "\n",
    "    # Create relations\n",
    "    relation_ids = []\n",
    "    for constraint in json_obj[\"DCRgraph\"][\"constraints\"]:\n",
    "        source_activity = constraint[\"source activity\"]\n",
    "        target_activity = constraint[\"target activity\"]\n",
    "        relation_id = f\"Relation_{uuid.uuid4().hex[:8]}\"  # Generate a unique relation ID\n",
    "\n",
    "        #check whether constraint type matches DCR Editor XML types\n",
    "        if constraint[\"constraint\"] == \"Condition\":\n",
    "            constraint_type = \"condition\"\n",
    "        elif constraint[\"constraint\"] == \"Response\":\n",
    "            constraint_type = \"response\"\n",
    "        elif constraint[\"constraint\"] == \"Exclude\":\n",
    "            constraint_type = \"exclude\"\n",
    "        elif constraint[\"constraint\"] == \"Include\":\n",
    "            constraint_type = \"include\"\n",
    "        elif constraint[\"constraint\"] == \"Milestone\":\n",
    "            constraint_type = \"milestone\"\n",
    "        else:\n",
    "            raise TypeError(\"Constraint Type was not identified correctly\")\n",
    "        \n",
    "        # Find the corresponding event IDs\n",
    "        source_event_id = next((eid for eid, act in event_ids.items() if act[\"activity\"] == source_activity), None)\n",
    "        target_event_id = next((eid for eid, act in event_ids.items() if act[\"activity\"] == target_activity), None)\n",
    "\n",
    "        if source_event_id and target_event_id:\n",
    "            # Create the dcr:relation with a unique ID\n",
    "            ET.SubElement(dcr_graph, \"dcr:relation\", {\n",
    "                \"id\": relation_id,\n",
    "                \"type\": constraint_type, \n",
    "                \"sourceRef\": source_event_id,\n",
    "                \"targetRef\": target_event_id\n",
    "            })\n",
    "            relation_ids.append(relation_id)  # Store the relation ID for later use\n",
    "\n",
    "    # Create dcrRootBoard element\n",
    "    root_board = ET.SubElement(definitions, \"dcrDi:dcrRootBoard\", id=\"RootBoard\")\n",
    "    plane = ET.SubElement(root_board, \"dcrDi:dcrPlane\", id=\"Plane\", boardElement=\"dcrGraph\")\n",
    "\n",
    "    # Create shapes for the diagram\n",
    "    for event_id in event_ids.keys():\n",
    "        shape = ET.SubElement(plane, \"dcrDi:dcrShape\", id=f\"{event_id}_di\", boardElement=event_id)\n",
    "        # Example bounds, you can adjust the coordinates as needed\n",
    "        ET.SubElement(shape, \"dc:Bounds\", x=\"640\", y=\"230\", width=\"130\", height=\"150\")\n",
    "\n",
    "    # Create dcrDi:relation elements with unique IDs\n",
    "    for relation_id in relation_ids:\n",
    "        # Create a corresponding dcrDi:relation\n",
    "        relation_di_id = f\"{relation_id}_di\"  # ID for dcrDi:relation\n",
    "        relation_di = ET.SubElement(plane, \"dcrDi:relation\", id=relation_di_id, boardElement=relation_id)\n",
    "        # Example waypoints, you can adjust the coordinates as needed\n",
    "        ET.SubElement(relation_di, \"dcrDi:waypoint\", x=\"705\", y=\"380\")\n",
    "        ET.SubElement(relation_di, \"dcrDi:waypoint\", x=\"705\", y=\"400\")\n",
    "        ET.SubElement(relation_di, \"dcrDi:waypoint\", x=\"620\", y=\"400\")\n",
    "        ET.SubElement(relation_di, \"dcrDi:waypoint\", x=\"620\", y=\"305\")\n",
    "        ET.SubElement(relation_di, \"dcrDi:waypoint\", x=\"640\", y=\"305\")\n",
    "\n",
    "    # Convert to string\n",
    "    xml_str = ET.tostring(definitions, encoding='UTF-8', xml_declaration=True).decode('UTF-8')\n",
    "    return xml_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd8f95",
   "metadata": {},
   "source": [
    "## Combined Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33aada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_run = 0\n",
    "generation_start_time = datetime.now()\n",
    "errors = 0\n",
    "role_times = []\n",
    "role_input_token = []\n",
    "role_output_token = []\n",
    "activity_times = []\n",
    "activity_input_token = []\n",
    "activity_output_token = []\n",
    "rule_times = []\n",
    "rule_input_token = []\n",
    "rule_output_token = []\n",
    "\n",
    "while True:\n",
    "    if current_run >= loop_limit:\n",
    "        # safety mechanism to ensure termination if an unexpected issue is not handled\n",
    "        # important to leave in when running everything automatically\n",
    "        raise ValueError(\"Exceeded maximumg number of runs. Generation of Graph failed.\")\n",
    "    try:\n",
    "        roles, role_time, role_input, role_output, activities, activity_time, activity_input, activity_output, rules, rule_time, rule_input, rule_output = runPromptPipeline()\n",
    "        role_times.append(role_time)\n",
    "        role_input_token.append(role_input)\n",
    "        role_output_token.append(role_output)\n",
    "        activity_times.append(activity_time)\n",
    "        activity_input_token.append(activity_input)\n",
    "        activity_output_token.append(activity_output)\n",
    "        rule_times.append(rule_time)\n",
    "        rule_input_token.append(rule_input)\n",
    "        rule_output_token.append(rule_output)\n",
    "        graphJSON = generateJSON(roles,activities,rules)\n",
    "        print(\"Parsing to JSON successful\")\n",
    "        xml = json_to_xml(graphJSON)\n",
    "        print(f\"XML Generation successful ({process_name},Pipeline_A1_{prompt_name})\")\n",
    "        print(\"xml:\", xml)\n",
    "        print(\"\\n\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        current_run += 1\n",
    "        errors += 1\n",
    "        print(f\"Graph Generation failed with error ({process_name},Pipeline_A1_{prompt_name})\")\n",
    "        print(e, flush=True)\n",
    "        runs_left = loop_limit - current_run\n",
    "        print(\"Iterations left: \" + str(runs_left))\n",
    "        print(\"\\n\")\n",
    "\n",
    "generation_finished_time = datetime.now() - generation_start_time\n",
    "generation_duration = generation_finished_time.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc92c49",
   "metadata": {},
   "source": [
    "## Save Time to Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_times = f\"{process_name};{generation_duration};{role_times};{role_input_token};{role_output_token};{activity_times};{activity_input_token};{activity_output_token};{rule_times};{rule_input_token};{rule_output_token};{errors}\\n\"\n",
    "\n",
    "folder = Path('./6_Time_Analysis') / \"data\" / large_language_model / \"Pipeline_A1\"\n",
    "folder.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "file_name = f\"{prompt_name}.txt\"\n",
    "file_path = folder / file_name \n",
    "\n",
    "with open(file_path, 'a+') as file:\n",
    "    file.write(new_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771cda2",
   "metadata": {},
   "source": [
    "## Save XML to Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(result_xml_path) / large_language_model / \"Pipeline_A1\" / prompt_name\n",
    "folder.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "file_name = f\"{process_name}_{large_language_model}_{prompt_name}_{datetime.now().date()}_{datetime.now().time()}.xml\"\n",
    "file_path = folder / file_name \n",
    "\n",
    "with open(file_path, 'w') as result_destination:\n",
    "    result_destination.write(xml)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DICE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
