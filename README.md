# Declarative Process Models from Texts via In-Context Learning

This repository includes the experimental setup and dataset for prompt engineering architectures that discover DCR graphs from business process specifications.

([go to setup and evaluation directly](#Evaluation))

## Prompt Engineering Architecture

The discovery pipelines in `3_Pipelines` combines the retrieval of additional information from documents, definition of prompts, construction of and requests to the LLM pipeline, the validation of the outputs and the generation of DCR graphs. 

The LLM pipeline is sequentially calling three prompts. We process the LLM's reply and use it as input in each succeeding prompt. We refer to Haystack (https://docs.haystack.deepset.ai/docs/pipelines) for additional information about the construction of the pipelines, including the setup of retrieval augmented generation (RGA) (https://docs.haystack.deepset.ai/docs/retrievers). 

### Pipelines

The three prompts are defined to discover the following elements in natural language process descriptions:
- **Role** = Entity performing an Activity; is assigned to one or more Activities
- **Event** = Activities that are executed or performed, and Events that happen (refered to as activities in the code)
- **Rule** = Deontic/Defeasible Constraint between two Events (We use the minimal types introduced in [[1]](#1): Condition, Response, Exclude, Include, Milestone)

Two different setups are used:

- **Pipeline A**: Instruct LLM to find Roles, then Activities, and then Rules based on previous findings. The Rules are to be found all at once.
- **Pipeline B**: Instruct LLM to find Roles, then Activities, and then Rules based on previous findings. Rules of one type are found independently of the other types (First Condition, next Response, next...). Previously discovered rules are not included in succeeding request to the LLM.

### Prompts

Five prompts were defined with gradually increasing details:

- **Prompt 1**: only uses simple context about the LLM's role and knowledge
- **Prompt 2**: adds in-context learning of DCR concepts and definitions
- **Prompt 3**: adds one correct example based on extracts of the annotated process descriptions (see Data/Gold Standard below)
- **Prompt 4**: adds another correct example and one wrong example
- **Prompt 5**: adds annotation guidelines to in-context learning


## Data

### Documents for Retrieval Augmented Generation (RAG) (`0_In-context_Learning`)

Two documents for RAG have been added.

The first document (`DCR_concepts.txt`) includes an introduction to declarative process modelling, and definitions of DCR elements as well as some of its concepts. The content is based on the documentation on DCR from DCR Solutions (https://dcrsolutions.net/) as well as related papers ([[2]](#2),[[3]](#3)).

The second document (`Annotation_Guidelines_DCR_Laws.pdf`) are annotation guidelines that have previously been defined for the extraction of processes from texts including circumstantial information in the legal sector.

### Business Process Specifications (`1_Process_Descriptions`)

The discovery pipeline extracts declarative specifications from ten business process specifications that are provided as natural language descriptions. Additional descriptions can be added in the corresponding folder.

### Gold Standard (`2_Gold_Standard`)

The evaluation for internal validation of the discovery pipeline is conducted as a comparison between elements of the generated DCR graphs and our gold standard. The gold standard is derived from annotating the ten process descriptions and modelling their graphs in DCR-js (https://github.com/hugoalopez-dtu/dcr-js).

### DCR graphs from the highlighter suggestions (`7_External_Validation`)

The evaluation for external validation of the discovery pipeline is conducted as a comparison between elements of the generated DCR graphs and graphs generated by an external tool. We used the highlighter tool ([[4]](#4)) that employs NLP to discover DCR elements in textual descriptions.

### Similarity Thresholds (`thresholds.ipynb`)

Evaluating the performance of the LLM required the calculation of semantic similarity between generated entities and the entities in the gold standard graphs. We defined thresholds for semantic similarity based on the exploratory setup in `thresholds.ipynb`. We utilize SBERT to calculate the similarity between entities (https://www.sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html).

The threshold for roles is currently set to 0.7. The threshold for events (/activities) is currently set to 0.4.

## Evaluation

### Setup and Dependencies

For our evaluation, we created an environment for python (3.13.2) with mamba (https://mamba.readthedocs.io/en/latest/index.html) and installed the following libraries and dependencies:

Every script is defined as a jupyter notebook, thus the `notebook` library is required. We also make use of common libraries like `os`, `numpy` and `path`.
Additional libraries for the graph generation and analysis require `json`, `xml`, `datetime` and `xmltodict`.

We refer to Haystack (https://docs.haystack.deepset.ai/docs/pipelines) for the requirements of the pipelines, including the setup of retrieval augmented generation (RGA) (https://docs.haystack.deepset.ai/docs/retrievers).

We use Ollama (https://github.com/ollama/ollama) to run the LLMs. It is very intuitive and let's one pull and run LLM locally. We used **Llama3.2 3B** and **Llama3.1 8B**.\
We refer to Haystack's `OllamaGenerator` (https://docs.haystack.deepset.ai/docs/ollamagenerator) for the usage within the pipeline. Other providers of LLMs can be used as well, e.g., HuggingFace with the `HuggingFaceAPIGenerator`(https://docs.haystack.deepset.ai/docs/generators). \
If GPU usage is prefered or required by larger models, additional tools, e.g., PyTorch with CUDA support, have to be installed as well. 

The similarity checks require `sentence-transformers` (https://www.sbert.net/docs/installation.html).

### Running the pipelines 

The pipelines that are defined in `3_Pipelines` are run with the `run_pipelines.ipynb` jupyter notebook.

The notebook includes global variables that refer to directory paths. It should therefore be run from the root directory where the notebook is located.\
Other variables are required by the called notebooks. `models` lists the models for which the evaluation will be run. The model name is passed on to the OllamaGenerator. In this context, `llm_url` is used to pass a defined url to Ollama, in case Ollama is deployed on an external server and not locally. `loop_limit` limits the number of repetitions for each attempt to generate a graph from one process description. 

To run all pipelines, one only has to start `run_pipelines.ipynb`. It calls all defined pipelines, for all prompts, with each process description, and runs them with all defined models.


#### Validation Loop

In order to successfully generate a DCR graph with LLMs, one has to be prepared for hallucinations. For our generation, we instruct the LLM to embed the reply within a JSON structure, that can be compiled to XML. However, we must validate the output and repeat the tasks if we were not able to generate the desired objects. 

Each implemented pipeline-prompt combination includes exception handling to react to errors thrown during the generation process. Errors as well as successful generations are printed to the cell output to monitor the process.

#### Generated DCR Graphs

Successfully generated DCR graphs are saved as XML files under `4_Generated_XML` / Model / Pipeline / Prompt. The file name is a combination of: Process Name + Model + Pipeline + Prompt + Date + Time.

#### Data on Time, Tokens and Errors

For every successful generation of a DCR graph, additional data for analysis purposes are saved under `6_Time_Analysis/data` / Model / Pipeline, and to a TXT file named after Pipeline + Prompt.

One example is the following:\
`4_Expense_Handling_Process;36.938616;[6.127018];[287];[13];[12.635021];[326];[78];[18.174572];[461];[139];0`

The data is to be read in the following way:
- `4_Expense_Handling_Process`: Process Name 
- `36.938616`: Total duration in seconds until the graph was generated
- `[6.127018];[287];[13]` - for Roles: duration (in seconds) for running LLM pipeline ; input tokens of prompt ; output tokens of reply
- `[12.635021];[326];[78]` - for Activities: duration (in seconds) for running LLM pipeline ; input tokens of prompt ; output tokens of reply
- `[18.174572];[461];[139]` - for Rules: duration (in seconds) for running LLM pipeline ; input tokens of prompt ; output tokens of reply
- `0`: Number of failed runs (errors encountered during the generation)

With every error, additional data from every repetition run is added to the arrays.

### Internal Validation (`f1_analysis_INTERNAL.ipynb`)

The internal validation is not run automatically and has to be started manually after the graphs have been generated.

Roles, activities and rules are extracted from the generated graphs and evaluated against the gold standard graphs of the same process. The scores for true-positive, true-negative, false-positive and false-negative are determined and combined for each prompting strategy (Pipeline + Prompt). Based on those scores, the performance is calculated for precision, recall and F1.

The results are saved to `5_Test_Results`.

### External Validation (`f1_analysis_EXTERNAL.ipynb`)

The procedure for the external validation is the same as for the internal validation, but evaluates the graphs from the external highlighter tool against the gold standard.

The results are saved to `7_External_Validation/External_Validation.txt`.

## Results

### Internal Validation 

The results for the initial experiments with Llama3.1-8B and Llama3.2-3B were run with access to 48 threads of two CPUs (EPYC 7302 16c/32t 3.0 GHz) and max. 512GB of RAM. Each LLM request had a time limit of 600s and the maximum number of repetitions to generate a graph was 30.

![Results of the internal validation](https://github.com/lindner-jonas/discovery_DCR_from_Text/tree/main/5_Test_Results/Results_Internal_Validation.png?raw=true)

### External Validation

![Results of the external validation](https://github.com/lindner-jonas/discovery_DCR_from_Text/tree/main/7_External_Validation/Results_External_Validation.png?raw=true)

## References
<a id="1">[1]</a>
T.T. Hildebrandt and R.R. Mukkamala,
“Declarative event-based workflow as distributed dynamic condition response graphs”, 
arXiv preprint arXiv:1110.4161,
2011

<a id="2">[2]</a>
 H.A. López, R. Strømsted, J.-M. Niyodusenga, and M. Marquard, 
 “Declarative process discovery: Linking process and textual views”, International Conference on Advanced Information Systems Engineering (CAISE), pp. 109–117, Springer, 2021.

<a id="3">[3]</a>
H.A. López, S. Debois, T. Slaats, and T.T. Hildebrandt, 
“Business process compliance using reference models of law”, 
Fundamental Approaches to Software Engineering LNCS 12076, p. 378.

<a id="4">[4]</a>
H.A. López, T. Hildebrandt, S. Debois, and M. Marquard, 
“The process highlighter: From texts to declarative processes and back”, 
in CEUR workshop proceedings, pp. 66–70, CEUR Workshop Proceedings, 2018.